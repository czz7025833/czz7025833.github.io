<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>使用kafka connect创建基于kafka的实时data pipeline | 陈豪的博客</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Hello, 我又来了，最近一直在搞和data pipeline相关的工作，所以就结合目前自己使用的技术来说明一下Kafka和Kafka Connect是如何构建一个data pipeline的。 什么是KafkaKafka是一个消息队列，现在的软件架构中，基本都是有消息队列存在的。消息队列一般有几个好处：  提供了一个异步的处理模式 解耦了组件之间的消息传递 消息代表了一种数据的实时的传递，为我">
<meta property="og:type" content="article">
<meta property="og:title" content="使用kafka connect创建基于kafka的实时data pipeline">
<meta property="og:url" content="http://yoursite.com/2020/03/12/%E4%BD%BF%E7%94%A8kafka-connect%E5%88%9B%E5%BB%BA%E5%9F%BA%E4%BA%8Ekafka%E7%9A%84%E5%AE%9E%E6%97%B6data-pipeline/index.html">
<meta property="og:site_name" content="陈豪的博客">
<meta property="og:description" content="Hello, 我又来了，最近一直在搞和data pipeline相关的工作，所以就结合目前自己使用的技术来说明一下Kafka和Kafka Connect是如何构建一个data pipeline的。 什么是KafkaKafka是一个消息队列，现在的软件架构中，基本都是有消息队列存在的。消息队列一般有几个好处：  提供了一个异步的处理模式 解耦了组件之间的消息传递 消息代表了一种数据的实时的传递，为我">
<meta property="og:locale" content="CN">
<meta property="article:published_time" content="2020-03-12T13:14:11.000Z">
<meta property="article:modified_time" content="2020-03-12T14:58:42.376Z">
<meta property="article:author" content="Hao Chen 陈豪">
<meta property="article:tag" content="Kafka">
<meta property="article:tag" content="Kafka Connect">
<meta property="article:tag" content="Data Pipeline">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="陈豪的博客" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 4.2.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">陈豪的博客</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-使用kafka-connect创建基于kafka的实时data-pipeline" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/03/12/%E4%BD%BF%E7%94%A8kafka-connect%E5%88%9B%E5%BB%BA%E5%9F%BA%E4%BA%8Ekafka%E7%9A%84%E5%AE%9E%E6%97%B6data-pipeline/" class="article-date">
  <time datetime="2020-03-12T13:14:11.000Z" itemprop="datePublished">2020-03-12</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      使用kafka connect创建基于kafka的实时data pipeline
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Hello, 我又来了，最近一直在搞和data pipeline相关的工作，所以就结合目前自己使用的技术来说明一下Kafka和Kafka Connect是如何构建一个data pipeline的。</p>
<h2 id="什么是Kafka"><a href="#什么是Kafka" class="headerlink" title="什么是Kafka"></a>什么是Kafka</h2><p><a href="http://kafka.apache.org/" target="_blank" rel="noopener">Kafka</a>是一个消息队列，现在的软件架构中，基本都是有消息队列存在的。消息队列一般有几个好处：</p>
<ul>
<li>提供了一个异步的处理模式</li>
<li>解耦了组件之间的消息传递</li>
<li>消息代表了一种数据的实时的传递，为我们做实时的数据流的相关操作提供了基础</li>
<li>等等，具体的好处可以查看消息队列相关的知识</li>
</ul>
<h2 id="什么是Kafka-Connect"><a href="#什么是Kafka-Connect" class="headerlink" title="什么是Kafka Connect"></a>什么是Kafka Connect</h2><p><a href="https://docs.confluent.io/current/connect/index.html" target="_blank" rel="noopener">Kafka Connect</a>是基于kafka消息队列的组件，主要目的是为了方便其他的系统和kafka系统之间的集成，所以kafka Connect相当于提供了一套接口，提供了一些常用的Connector的组件，以数据流的方式来集成外部数据</p>
<h2 id="什么是Data-Pipeline"><a href="#什么是Data-Pipeline" class="headerlink" title="什么是Data Pipeline"></a>什么是Data Pipeline</h2><p>Data Pipeline是一个比ETL更加宽泛的概念，ETL以往比较常用的概念就是把数据以batch的方式从一个系统集成到另外一个系统，中间会有一个数据转换的过程，那么Data Pipeline可以说是代表了一个数据的流动的管道，可以是很多系统之间数据的集成，方式可以是流的方式，也可以是批的方式，只要代表了数据在系统之间的流动，都可以以Data Pipeline的概念来表达。</p>
<hr>
<h2 id="安装Kafka"><a href="#安装Kafka" class="headerlink" title="安装Kafka"></a>安装Kafka</h2><ol>
<li>Kafka的版本选择和下载</li>
</ol>
<p>Kafka的版本在0.XX和2.XX中会有很大的变化，包括一些API和一些工具，以及它自身的实现方式都有很大的区别。这篇博客会以最新的Kafka版本2.4.0做演示，如果读者是用以前的Kafka的版本，可能有些功能并不一定相同。<a href="https://www.apache.org/dyn/closer.cgi?path=/kafka/2.4.0/kafka_2.12-2.4.0.tgz" target="_blank" rel="noopener">kafka下载</a>, 选择的scala版本是2.12，官方也是这么推荐的，我们要做的与scala没啥关系，所以任意版本都可以。</p>
<ol start="2">
<li>Kafka配置和启动</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf kafka_2.12-2.4.0.tgz -C ~</span><br><span class="line">cd kafka_2.12-2.4.0/</span><br></pre></td></tr></table></figure>
<p>解压缩Kafka安装包到指定的位置，我这里是到我自己的home目录</p>
<p>Kafka默认配置文件是config/server.properties, Zookeeper默认配置文件是config/zookeeper.properties</p>
<p>Kafka的启动一定需要Zookeeper的启动，所以我们先启动Zookeeper</p>
<p>我们直接以默认的配置启动</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">./bin/zookeeper-server-start.sh config/zookeeper.properties</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line">[2020-03-12 22:03:54,458] INFO Created server with tickTime 3000 minSessionTimeout 6000 maxSessionTimeout 60000 datadir /tmp/zookeeper/version-2 snapdir /tmp/zookeeper/version-2 (org.apache.zookeeper.server.ZooKeeperServer)</span><br><span class="line">[2020-03-12 22:03:54,484] INFO Using org.apache.zookeeper.server.NIOServerCnxnFactory as server connection factory (org.apache.zookeeper.server.ServerCnxnFactory)</span><br><span class="line">[2020-03-12 22:03:54,490] INFO Configuring NIO connection handler with 10s sessionless connection timeout, 1 selector thread(s), 8 worker threads, and 64 kB direct buffers. (org.apache.zookeeper.server.NIOServerCnxnFactory)</span><br><span class="line">[2020-03-12 22:03:54,512] INFO binding to port 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.NIOServerCnxnFactory)</span><br><span class="line">[2020-03-12 22:03:54,543] INFO zookeeper.snapshotSizeFactor = 0.33 (org.apache.zookeeper.server.ZKDatabase)</span><br><span class="line">[2020-03-12 22:03:54,546] INFO Snapshotting: 0x0 to /tmp/zookeeper/version-2/snapshot.0 (org.apache.zookeeper.server.persistence.FileTxnSnapLog)</span><br><span class="line">[2020-03-12 22:03:54,555] INFO Snapshotting: 0x0 to /tmp/zookeeper/version-2/snapshot.0 (org.apache.zookeeper.server.persistence.FileTxnSnapLog)</span><br><span class="line">[2020-03-12 22:03:54,598] INFO Using checkIntervalMs=60000 maxPerMinute=10000 (org.apache.zookeeper.server.ContainerManager)</span><br></pre></td></tr></table></figure>
<p>Zookeeper 成功启动，同时它会监听默认端口2181，新开一个窗口，开始以默认的配置启动Kafka</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">./bin/kafka-server-start.sh config/server.properties</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line">[2020-03-12 22:05:46,399] INFO [GroupCoordinator 0]: Startup complete. (kafka.coordinator.group.GroupCoordinator)</span><br><span class="line">[2020-03-12 22:05:46,408] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 9 milliseconds. (kafka.coordinator.group.GroupMetadataManager)</span><br><span class="line">[2020-03-12 22:05:46,430] INFO [ProducerId Manager 0]: Acquired new producerId block (brokerId:0,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1 (kafka.coordinator.transaction.ProducerIdManager)</span><br><span class="line">[2020-03-12 22:05:46,606] INFO [TransactionCoordinator id=0] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)</span><br><span class="line">[2020-03-12 22:05:46,609] INFO [Transaction Marker Channel Manager 0]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)</span><br><span class="line">[2020-03-12 22:05:46,610] INFO [TransactionCoordinator id=0] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)</span><br><span class="line">[2020-03-12 22:05:46,658] INFO [ExpirationReaper-0-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)</span><br><span class="line">[2020-03-12 22:05:46,690] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)</span><br><span class="line">[2020-03-12 22:05:46,707] INFO [SocketServer brokerId=0] Started data-plane processors for 1 acceptors (kafka.network.SocketServer)</span><br><span class="line">[2020-03-12 22:05:46,717] INFO Kafka version: 2.4.0 (org.apache.kafka.common.utils.AppInfoParser)</span><br><span class="line">[2020-03-12 22:05:46,718] INFO Kafka commitId: 77a89fcf8d7fa018 (org.apache.kafka.common.utils.AppInfoParser)</span><br><span class="line">[2020-03-12 22:05:46,718] INFO Kafka startTimeMs: 1584021946708 (org.apache.kafka.common.utils.AppInfoParser)</span><br><span class="line">[2020-03-12 22:05:46,721] INFO [KafkaServer id=0] started (kafka.server.KafkaServer)</span><br></pre></td></tr></table></figure>
<p>Kafka启动成功，默认监听端口是9092</p>
<ol start="3">
<li>Kafka Connect配置和启动Source Connector</li>
</ol>
<p>Kafka Connect有两种启动模式，一种是standalone模式，我们就会以这种模式演示，为了简单方便，还有一种是distributed模式启动，如果要上生产，一般都是以distributed方式启动。</p>
<p>Kafka Connect的默认配置文件在confg/connect-standalone.properties， 我们使用内置的一个Source Connector(org.apache.kafka.connect.file.FileStreamSourceConnector), 对应的默认配置文件在config/connect-console-source.properties， 这个connector会读取我们console的输入，然后发送到kafka队列里面</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">./bin/connect-standalone.sh config/connect-standalone.properties config/connect-console-source.properties</span><br><span class="line">...</span><br><span class="line">	ssl.provider = null</span><br><span class="line">	ssl.secure.random.implementation = null</span><br><span class="line">	ssl.trustmanager.algorithm = PKIX</span><br><span class="line">	ssl.truststore.location = null</span><br><span class="line">	ssl.truststore.password = null</span><br><span class="line">	ssl.truststore.type = JKS</span><br><span class="line">	transaction.timeout.ms = 60000</span><br><span class="line">	transactional.id = null</span><br><span class="line">	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer</span><br><span class="line"> (org.apache.kafka.clients.producer.ProducerConfig:347)</span><br><span class="line">[2020-03-12 22:41:56,962] INFO Kafka version: 2.4.0 (org.apache.kafka.common.utils.AppInfoParser:117)</span><br><span class="line">[2020-03-12 22:41:56,962] INFO Kafka commitId: 77a89fcf8d7fa018 (org.apache.kafka.common.utils.AppInfoParser:118)</span><br><span class="line">[2020-03-12 22:41:56,964] INFO Kafka startTimeMs: 1584024116962 (org.apache.kafka.common.utils.AppInfoParser:119)</span><br><span class="line">[2020-03-12 22:41:56,984] INFO WorkerSourceTask&#123;id=local-console-source-0&#125; Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask:209)</span><br><span class="line">[2020-03-12 22:41:56,985] INFO Created connector local-console-source (org.apache.kafka.connect.cli.ConnectStandalone:112)</span><br><span class="line">[2020-03-12 22:41:57,063] INFO [Producer clientId=connector-producer-local-console-source-0] Cluster ID: QgewGhJBQVypKGSR3FbnHg (org.apache.kafka.clients.Metadata:261)</span><br><span class="line">abc</span><br><span class="line">[2020-03-12 22:42:27,000] INFO WorkerSourceTask&#123;id=local-console-source-0&#125; Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:416)</span><br><span class="line">[2020-03-12 22:42:27,000] INFO WorkerSourceTask&#123;id=local-console-source-0&#125; flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:433)</span><br><span class="line">test123</span><br><span class="line">[2020-03-12 22:42:37,005] INFO WorkerSourceTask&#123;id=local-console-source-0&#125; Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:416)</span><br><span class="line">[2020-03-12 22:42:37,006] INFO WorkerSourceTask&#123;id=local-console-source-0&#125; flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:433)</span><br><span class="line">[2020-03-12 22:42:37,014] INFO WorkerSourceTask&#123;id=local-console-source-0&#125; Finished commitOffsets successfully in 8 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:515)</span><br></pre></td></tr></table></figure>
<p>可以看到log上面写了成功创建了local-console-source的connector，同时我输入了abc， test123在console里面，我们去查看Kafka对应的消息是否收到</p>
<ol start="4">
<li>查看Kafka消息</li>
</ol>
<p>Kafka自带了很多工具的脚本，方便我们对Kafka做一些操作，比如查看topic，消费消息之类的</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">./bin/kafka-topics.sh --zookeeper localhost:2181 --list</span><br><span class="line">connect-test</span><br></pre></td></tr></table></figure>
<p>有一个topic叫做connect-test已经被创建了，该topic的名称是声明在connect-console-source.properties这个默认的配置文件里的，所有这个connector数据都会进入到这个topic里面</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">./bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic connect-test --from-beginning</span><br><span class="line">&#123;"schema":&#123;"type":"string","optional":false&#125;,"payload":"12"&#125;</span><br><span class="line">&#123;"schema":&#123;"type":"string","optional":false&#125;,"payload":"13"&#125;</span><br><span class="line">&#123;"schema":&#123;"type":"string","optional":false&#125;,"payload":"abc"&#125;</span><br><span class="line">&#123;"schema":&#123;"type":"string","optional":false&#125;,"payload":"test123"&#125;</span><br></pre></td></tr></table></figure>
<p>我们使用kafka-console-consumer来从头消费这个topic的数据，可以看到我们刚才输入的abc和test123都已经在消息里面了，前面的两条是我之前的测试数据，不用管它 :)</p>
<ol start="5">
<li>启动Sink Connector</li>
</ol>
<p>我们使用内置的一个Sink Connector(org.apache.kafka.connect.file.FileStreamSinkConnector), 对应的默认配置文件在config/connect-console-sink.properties， 这个connector会把kafka中的消息打印到我们的console</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">./bin/connect-standalone.sh config/connect-standalone.properties config/connect-console-sink.properties</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line">nternals.AbstractCoordinator:533)</span><br><span class="line">[2020-03-12 22:52:23,511] INFO [Consumer clientId=connector-consumer-local-console-sink-0, groupId=connect-local-console-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:533)</span><br><span class="line">[2020-03-12 22:52:23,519] INFO [Consumer clientId=connector-consumer-local-console-sink-0, groupId=connect-local-console-sink] Finished assignment for group at generation 1: &#123;connector-consumer-local-console-sink-0-27735255-dd48-4759-b721-594d420b3241=org.apache.kafka.clients.consumer.ConsumerPartitionAssignor$Assignment@2785762f&#125; (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:585)</span><br><span class="line">[2020-03-12 22:52:23,526] INFO [Consumer clientId=connector-consumer-local-console-sink-0, groupId=connect-local-console-sink] Successfully joined group with generation 1 (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:484)</span><br><span class="line">[2020-03-12 22:52:23,625] INFO [Consumer clientId=connector-consumer-local-console-sink-0, groupId=connect-local-console-sink] Adding newly assigned partitions: connect-test-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:267)</span><br><span class="line">[2020-03-12 22:52:23,650] INFO [Consumer clientId=connector-consumer-local-console-sink-0, groupId=connect-local-console-sink] Found no committed offset for partition connect-test-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1241)</span><br><span class="line">[2020-03-12 22:52:23,721] INFO [Consumer clientId=connector-consumer-local-console-sink-0, groupId=connect-local-console-sink] Resetting offset for partition connect-test-0 to offset 0. (org.apache.kafka.clients.consumer.internals.SubscriptionState:385)</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">abc</span><br><span class="line">test123</span><br></pre></td></tr></table></figure>
<p>可以看到console上面显示了topic中我们的信息，接下来我们把source和sink结合起来，是整个流程变成一个数据流，数据从我们输入到console，经过kafka，最后重新打印回console，形成一个闭环，最终完成一个简易的data pipeline。</p>
<ol start="6">
<li>Data Pipeline的流程</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">./bin/connect-standalone.sh config/connect-standalone.properties config/connect-console-sink.properties config/connect-console-source.properties</span><br><span class="line">...</span><br><span class="line">[2020-03-12 22:55:00,392] INFO Kafka version: 2.4.0 (org.apache.kafka.common.utils.AppInfoParser:117)</span><br><span class="line">[2020-03-12 22:55:00,393] INFO Kafka commitId: 77a89fcf8d7fa018 (org.apache.kafka.common.utils.AppInfoParser:118)</span><br><span class="line">[2020-03-12 22:55:00,394] INFO Kafka startTimeMs: 1584024900392 (org.apache.kafka.common.utils.AppInfoParser:119)</span><br><span class="line">[2020-03-12 22:55:00,400] INFO WorkerSourceTask&#123;id=local-console-source-0&#125; Source task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSourceTask:209)</span><br><span class="line">[2020-03-12 22:55:00,401] INFO Created connector local-console-source (org.apache.kafka.connect.cli.ConnectStandalone:112)</span><br><span class="line">[2020-03-12 22:55:00,489] INFO [Producer clientId=connector-producer-local-console-source-0] Cluster ID: QgewGhJBQVypKGSR3FbnHg (org.apache.kafka.clients.Metadata:261)</span><br><span class="line">[2020-03-12 22:55:10,405] INFO WorkerSourceTask&#123;id=local-console-source-0&#125; Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:416)</span><br><span class="line">[2020-03-12 22:55:10,406] INFO WorkerSourceTask&#123;id=local-console-source-0&#125; flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:433)</span><br><span class="line">hello1[2020-03-12 22:55:20,407] INFO WorkerSourceTask&#123;id=local-console-source-0&#125; Committing offsets (org.apache.kafka.connect.runtime.WorkerSourceTask:416)</span><br><span class="line">[2020-03-12 22:55:20,407] INFO WorkerSourceTask&#123;id=local-console-source-0&#125; flushing 0 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:433)</span><br><span class="line"></span><br><span class="line">hello1</span><br><span class="line">hello3</span><br><span class="line">hello3</span><br></pre></td></tr></table></figure>

<p>可以看到我在console上面输入hello1，console随即就会返回hello1给我，数据由kafka connect的source从console流入kafka，再由kafka connect的sink流回console， 形成了一个实时的小的data pipeline。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Kafka Connector提供了一套方案，用于集成Kafka和其他系统的数据，为Kafka想要最终成为一个基于消息流的流计算和处理平台提供了良好的基础</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/03/12/%E4%BD%BF%E7%94%A8kafka-connect%E5%88%9B%E5%BB%BA%E5%9F%BA%E4%BA%8Ekafka%E7%9A%84%E5%AE%9E%E6%97%B6data-pipeline/" data-id="ck7ovorv40000zxlxexso9xgr" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Data-Pipeline/" rel="tag">Data Pipeline</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kafka/" rel="tag">Kafka</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kafka-Connect/" rel="tag">Kafka Connect</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2020/02/20/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8Hexo%E5%92%8CGithub-Pages%E5%85%8D%E8%B4%B9%E6%90%AD%E5%BB%BABlog/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">如何使用Hexo和Github Pages免费搭建博客</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Data-Pipeline/" rel="tag">Data Pipeline</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Github-Pages/" rel="tag">Github Pages</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hexo/" rel="tag">Hexo</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Kafka/" rel="tag">Kafka</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Kafka-Connect/" rel="tag">Kafka Connect</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%8D%9A%E5%AE%A2/" rel="tag">博客</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Data-Pipeline/" style="font-size: 10px;">Data Pipeline</a> <a href="/tags/Github-Pages/" style="font-size: 10px;">Github Pages</a> <a href="/tags/Hexo/" style="font-size: 10px;">Hexo</a> <a href="/tags/Kafka/" style="font-size: 10px;">Kafka</a> <a href="/tags/Kafka-Connect/" style="font-size: 10px;">Kafka Connect</a> <a href="/tags/%E5%8D%9A%E5%AE%A2/" style="font-size: 10px;">博客</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/03/">March 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">February 2020</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/03/12/%E4%BD%BF%E7%94%A8kafka-connect%E5%88%9B%E5%BB%BA%E5%9F%BA%E4%BA%8Ekafka%E7%9A%84%E5%AE%9E%E6%97%B6data-pipeline/">使用kafka connect创建基于kafka的实时data pipeline</a>
          </li>
        
          <li>
            <a href="/2020/02/20/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8Hexo%E5%92%8CGithub-Pages%E5%85%8D%E8%B4%B9%E6%90%AD%E5%BB%BABlog/">如何使用Hexo和Github Pages免费搭建博客</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 Hao Chen 陈豪<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div>
</body>
</html>